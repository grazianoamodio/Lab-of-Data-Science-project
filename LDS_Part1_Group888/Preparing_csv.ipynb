{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing CSV and tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percorso del file CSV principale. Il seguente file è l'unione sulla chiave della data dei file \"police.csv\" e \"data.xml\"\n",
    "\n",
    "input_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essendo un notebook per premettermi di runnare anche le singole celle \"input_csv_file\" compare più volte. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gun table \n",
    "gun_table è la prima tabella che estraiamo. Ci aggiungiamo un gun_id da 0 alla lunghezza della tabella e anche una colonna \"Is_Stolen\" che mi restisce per ogni riga il valore di 1 se l'arma è stata rubata, 0 viceversa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione e creazione della tabella completate.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "#input_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'\n",
    "\n",
    "def estrai_crea_tabella(input_file, output_file, selected_columns):\n",
    "    # Leggi il file CSV principale e ottieni l'intestazione\n",
    "    with open(input_file, 'r') as main_csv:\n",
    "        main_reader = csv.DictReader(main_csv)\n",
    "        main_header = main_reader.fieldnames\n",
    "\n",
    "        # Seleziono le colonne che mi interessano: \n",
    "        selected_columns.extend(['gun_stolen', 'gun_type'])\n",
    "\n",
    "        # Rimuovo duplicati da selected_columns, perché è successo che mi restituisse duplicati nell'header. \n",
    "        selected_columns = list(set(selected_columns))\n",
    "\n",
    "        # Utilizzo un set per tracciare chiavi uniche\n",
    "        unique_keys = set()\n",
    "\n",
    "        # Aggiungo una chiave artificiale (ID) e creo la nuova tabella\n",
    "        extracted_rows = []\n",
    "        for i, row in enumerate(main_reader):\n",
    "            gun_id = i\n",
    "            new_row = {'gun_id': gun_id}\n",
    "            for col in selected_columns:\n",
    "                new_row[col] = row[col]\n",
    "\n",
    "            # Aggiungo la colonna 'Is_stolen' (1 if stolen, 0 else)\n",
    "            new_row['Is_stolen'] = 1 if row['gun_stolen'] == \"Stolen\" else 0\n",
    "\n",
    "            # Verifica duplicati\n",
    "            key = tuple(new_row.items())\n",
    "            if key not in unique_keys:\n",
    "                extracted_rows.append(new_row)\n",
    "                unique_keys.add(key)\n",
    "\n",
    "    # Scrivo la nuova tabella in un file CSV\n",
    "    with open(output_file, 'w', newline='') as output_csv:\n",
    "        output_columns = ['gun_id'] + ['Is_stolen'] + selected_columns    # gun_id come prima colonna\n",
    "        csv_writer = csv.DictWriter(output_csv, fieldnames=output_columns)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(extracted_rows)\n",
    "\n",
    "# Percorso file appena creato\n",
    "output_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/gun_table.csv'\n",
    "\n",
    "# Colonne desiderate da estrarre\n",
    "selected_columns_to_extract = ['gun_stolen', 'gun_type']\n",
    "\n",
    "# Esegui l'estrazione e la creazione della nuova tabella\n",
    "estrai_crea_tabella(input_csv_file, output_csv_file, selected_columns_to_extract)\n",
    "\n",
    "print(\"Estrazione e creazione della tabella completate.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### participant table \n",
    "uso lo stesso codice e metodo che ho usato per gun_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione e creazione della tabella completate.\n"
     ]
    }
   ],
   "source": [
    "# PARTICIPANT TABLE\n",
    "\n",
    "import csv\n",
    "\n",
    "def estrai_crea_tabella(input_file, output_file, selected_columns):\n",
    "    # Leggi il file CSV principale e ottieni l'intestazione\n",
    "    with open(input_file, 'r') as main_csv:\n",
    "        main_reader = csv.DictReader(main_csv)\n",
    "        main_header = main_reader.fieldnames\n",
    "\n",
    "    \n",
    "        selected_columns.extend(['participant_age_group','participant_gender','participant_status', 'participant_type'])  # colonne da aggiungere a part. table\n",
    "\n",
    "        selected_columns = list(set(selected_columns))\n",
    "\n",
    "        unique_keys = set()\n",
    "\n",
    "        extracted_rows = []\n",
    "        for i, row in enumerate(main_reader):\n",
    "            partecipant_id = i\n",
    "            new_row = {'participant_id': partecipant_id}\n",
    "            for col in selected_columns:\n",
    "                new_row[col] = row[col]\n",
    "\n",
    "            \n",
    "            key = tuple(new_row.items())\n",
    "            if key not in unique_keys:\n",
    "                extracted_rows.append(new_row)\n",
    "                unique_keys.add(key)\n",
    "\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as output_csv:\n",
    "        output_columns = ['participant_id'] + selected_columns  # participant_id come prima colonna\n",
    "        csv_writer = csv.DictWriter(output_csv, fieldnames=output_columns)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(extracted_rows)\n",
    "\n",
    "\n",
    "#input_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'\n",
    "output_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/participant_table.csv'\n",
    "selected_columns_to_extract = ['participant_age_group','participant_gender','participant_status', 'participant_type']\n",
    "estrai_crea_tabella(input_csv_file, output_csv_file, selected_columns_to_extract)\n",
    "print(\"Estrazione e creazione della tabella completate.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geography table \n",
    "Anche in questo caso uso lo stesso codice usato precedentemente. In questo caso però salvo una tabella geo ridotta. Infatti adesso salvo solamente la proiezione di latitude e longitude ed aggiungo l'id. Successivamente uso questa tabella ridotta per acquisire anche la città e lo stato localizzati a quelle coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrazione e creazione della tabella completate.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def estrai_crea_tabella(input_file, output_file, selected_columns):\n",
    "   \n",
    "    with open(input_file, 'r') as main_csv:\n",
    "        main_reader = csv.DictReader(main_csv)\n",
    "        main_header = main_reader.fieldnames\n",
    "\n",
    "        \n",
    "        selected_columns.extend(['latitude', 'longitude'])  # colonne da aggiungere alla table\n",
    "\n",
    "        selected_columns = list(set(selected_columns))\n",
    "\n",
    "        unique_keys = set()\n",
    "\n",
    "        extracted_rows = []\n",
    "        for i, row in enumerate(main_reader):\n",
    "            geo_id = i\n",
    "            new_row = {'geo_id': geo_id}\n",
    "            for col in selected_columns:\n",
    "                new_row[col] = row[col]\n",
    "\n",
    "            key = tuple(new_row.items())\n",
    "            if key not in unique_keys:\n",
    "                extracted_rows.append(new_row)\n",
    "                unique_keys.add(key)\n",
    "\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as output_csv:\n",
    "        output_columns = ['geo_id'] + selected_columns  # geo_id come prima colonna\n",
    "        csv_writer = csv.DictWriter(output_csv, fieldnames=output_columns)\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(extracted_rows)\n",
    "\n",
    "\n",
    "#input_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'\n",
    "output_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/geo_table_ridotta.csv'\n",
    "selected_columns_to_extract = ['latitude', 'longitude']\n",
    "estrai_crea_tabella(input_csv_file, output_csv_file, selected_columns_to_extract)\n",
    "print(\"Estrazione e creazione della tabella completate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geo_table continuazione\n",
    "\n",
    "import csv\n",
    "\n",
    "# Definiamo una funzione per calcolare la distanza tra due coordinate differenti che passeremo. \n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    lat_diff = abs(lat1 - lat2)\n",
    "    lon_diff = abs(lon1 - lon2)\n",
    "    total_diff = lat_diff + lon_diff\n",
    "    return total_diff\n",
    "\n",
    "# Leggo i dati dal csv contenente le latitudini e longitudini associate ad id (geo_table_ridotta)\n",
    "localizzazioni = []\n",
    "with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/geo_table_ridotta.csv', 'r') as geo_table_ridotta:\n",
    "    csv_reader = csv.DictReader(geo_table_ridotta)\n",
    "    for row in csv_reader:\n",
    "        localizzazioni.append(row)\n",
    "\n",
    "# Leggo i dati dal csv contenente le latitudini e longitudini associate alle città (table esterna)\n",
    "raccolta_esterna = []\n",
    "with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/uscities.csv', 'r') as esterno_file:\n",
    "    csv_reader = csv.DictReader(esterno_file)\n",
    "    for row in csv_reader:\n",
    "        raccolta_esterna.append(row)\n",
    "\n",
    "# Inizializzo il contenitore dei risultati sulle distanze tra posizioni\n",
    "result_data = []\n",
    "\n",
    "# Itero attraverso le righe di geo_table_ridotta\n",
    "for row2 in localizzazioni:\n",
    "    min_difference = float('inf')\n",
    "    min_city_data = None\n",
    "\n",
    "    for row1 in raccolta_esterna:\n",
    "        lat1, lon1 = float(row1['lat']), float(row1['lng'])\n",
    "        lat2, lon2 = float(row2['latitude']), float(row2['longitude'])\n",
    "\n",
    "        differenza_loc = calculate_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "        # Se la differenza tra le posizioni è minore della minima registrata, aggiorno i valori. (perché più vicini)\n",
    "        if differenza_loc < min_difference:\n",
    "            min_difference = differenza_loc\n",
    "            min_city_data = {\n",
    "                'city': row1['city'],\n",
    "                'state_id': row1['state_id'],\n",
    "                'state_name': row1['state_name']         \n",
    "            }\n",
    "\n",
    "    # Aggiungo le colonne richieste al risultato definitivo\n",
    "    result_row = {\n",
    "        'geo_id': row2['geo_id'],\n",
    "        'latitude': row2['latitude'],\n",
    "        'longitude': row2['longitude'],\n",
    "        'city': min_city_data['city'],\n",
    "        'state_id': min_city_data['state_id'],\n",
    "        'state_name': min_city_data['state_name']\n",
    "    }\n",
    "\n",
    "    result_data.append(result_row)\n",
    "\n",
    "# Salvo tutto\n",
    "with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/geo_table.csv', 'w', newline='') as output_file:\n",
    "    fieldnames = result_data[0].keys()\n",
    "    csv_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(result_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date table \n",
    "Usiamo lo stesso metodo usato precedentemente per le altre tabelle. In questo caso con datetime riusciamo ad aggiungerci anche altre informazioni dalla data, come il giorno, il mese, il quarter, giorno della settimana... Abbiamo rimosso l'ora dalla data perché si riferiva sempre alle \"00.00.00\". Ma lo abbiamo aggiunto in un'altra colonna in modo che se in futuro il dataset venisse integrato con orari differenti, vi è modo per aggiungerli. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I risultati sono stati scritti in /Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/date_table.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def process_dates(input_file_path, output_file_path):\n",
    "    \n",
    "    with open(input_file_path, 'r') as input_file:\n",
    "        reader = csv.DictReader(input_file)\n",
    "        original_data = list(reader)\n",
    "\n",
    "    # Creiamo un nuovo set vuoto, con un contatore inizializzato a 0 (partiamo da 0 per tutti gli id) \n",
    "    new_data = []\n",
    "    date_id_counter = 0\n",
    "\n",
    "    for row in original_data:\n",
    "        # Estraggo la colonna \"date\"\n",
    "        date = row['date']\n",
    "        \n",
    "        # Rimuovo l'orario dalla data per \"pulire\" il campo\n",
    "        date_obj = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "        date_no_time = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Aggiungo i dati nella nuova tabella\n",
    "        new_row = {\n",
    "            'date_id': date_id_counter,\n",
    "            'date': date_no_time,\n",
    "            \n",
    "            'day': date_obj.day,\n",
    "            'month': date_obj.month,\n",
    "            'year': date_obj.year,\n",
    "            'quarter': (date_obj.month - 1) // 3 + 1,\n",
    "            'day_of_week': date_obj.strftime('%A'),\n",
    "            'hour': date_obj.strftime('%H,%M,%S')\n",
    "        }\n",
    "        new_data.append(new_row)\n",
    "        date_id_counter += 1\n",
    "\n",
    "    # Scrivo i risultati nel file CSV che sarà la mia table \n",
    "    fieldnames = ['date_id', 'date', 'day', 'month', 'year', 'quarter', 'day_of_week', 'hour']\n",
    "\n",
    "    with open(output_file_path, 'w', newline='') as output_file:\n",
    "        writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(new_data)\n",
    "\n",
    "    print(f\"I risultati sono stati scritti in {output_file_path}\")\n",
    "\n",
    "#input_csv_file = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'\n",
    "output_csv_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/date_table.csv'\n",
    "# Eseguo\n",
    "process_dates(input_csv_file, output_csv_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### incident table \n",
    "In realtà la andiamo ad eliminare e accorpiamo incident_id direttamente a custodies. Questa scelta è dettata dal fatto che sarebbe inutile creare un altra chiave, useremo come chiave custody_id (incident_id non è chiave) mi indica \"chi\" ha commesso il reato. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custody table \n",
    "Per la  creazione di questa tabella dobbiamo associare i valori mappati in 3 dizionari differenti dalla tabella participant_table. Come nel caso della geo_table io mi creo una tabella provvisoria avente l'indice crime_gravity calcolato moltiplicando per ogni riga i valori mappati con i dizionari (quindi trasformati in int) di alcune colonne che riguardano l'età, il tipo e lo status dei partecipanti. \n",
    "Quindi vado ad aggiungere una colonna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completato.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def aggiungi_colonna(csv_reader, value_mapping, colonna_originale, nuova_colonna):\n",
    "    # Creo una nuova colonna con i valori integer associati\n",
    "    for row in csv_reader:\n",
    "        # Ottengo il valore dalla colonna esistente\n",
    "        original_value = row[colonna_originale]\n",
    "\n",
    "        # Trovo il corrispondente valore integer dal dizionario\n",
    "        integer_value = value_mapping.get(original_value, None)\n",
    "\n",
    "        # Aggiungi il nuovo valore alla nuova colonna\n",
    "        row[nuova_colonna] = integer_value\n",
    "\n",
    "        yield row\n",
    "\n",
    "# Leggi il file CSV\n",
    "with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/participant_table.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "    # Carico i dizionari\n",
    "    with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/dict_partecipant_age.json', 'r') as json_file:\n",
    "        value_mapping1 = json.load(json_file)\n",
    "\n",
    "    with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/dict_partecipant_status.json', 'r') as json_file:\n",
    "        value_mapping2 = json.load(json_file)\n",
    "\n",
    "    with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/dict_partecipant_type.json', 'r') as json_file:\n",
    "        value_mapping3 = json.load(json_file)\n",
    "\n",
    "    # Creazione di una lista contenente solo la colonna 'crime_gravity'\n",
    "    crime_gravity_list = []\n",
    "\n",
    "    # Applico la funzione per la colonna e dizionario associati \n",
    "    csv_reader1 = aggiungi_colonna(csv_reader, value_mapping1, 'participant_age_group', 'partecipant_age_dict')\n",
    "    csv_reader2 = aggiungi_colonna(csv_reader1, value_mapping2, 'participant_status', 'partecipant_status_dict')\n",
    "    csv_reader3 = aggiungi_colonna(csv_reader2, value_mapping3, 'participant_type', 'partecipant_type_dict')\n",
    "\n",
    "    # Calcolo la colonna crime_gravity e la aggiungo alla lista\n",
    "    for row in csv_reader3:\n",
    "        Crime_gravity = row['partecipant_age_dict'] * row['partecipant_status_dict'] * row['partecipant_type_dict']\n",
    "        crime_gravity_list.append({'crime_gravity': Crime_gravity})\n",
    "\n",
    "# Nuova tabella crime_gravity\n",
    "with open('/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/crime_gravity_table.csv', 'w', newline='') as new_csv_file:\n",
    "    fieldnames = ['crime_gravity']\n",
    "    csv_writer = csv.DictWriter(new_csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(crime_gravity_list)\n",
    "\n",
    "print(\"Completato.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adesso vado a crearmi la mia tabella custody_id andando ad aggiungere le tabelle degli id (le chiavi) delle altre tabelle precedentemente create. Questa nuova tabella avrà come chiave la colonna 'custody_id' che è presente ne file principale 'police_date' (il mio input_file_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completato.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Definisci i percorsi dei tuoi file CSV\n",
    "file1_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/police_date.csv'\n",
    "file2_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/incident_id_C.csv'\n",
    "file3_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/gun_table.csv'\n",
    "file4_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/participant_table.csv'\n",
    "file5_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/geo_table.csv'\n",
    "file6_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/date_table.csv'\n",
    "file7_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/crime_gravity_table.csv'\n",
    "# Specifica i nomi delle colonne da selezionare da ciascun file\n",
    "column_names = ['custody_id', 'incident_id', 'gun_id', 'partecipant_id', 'geo_id', 'date_id', 'crime_gravity']\n",
    "\n",
    "# Specifica il percorso del nuovo file CSV che vuoi creare\n",
    "new_file_path = '/Volumes/Graziano Amodio/Data science - esami /LDS/LDS_Project_2023/Preparing_csv_gra/custody_table.csv'\n",
    "\n",
    "# Apri i file CSV esistenti in modalità lettura\n",
    "with open(file1_path, 'r') as file1, \\\n",
    "        open(file2_path, 'r') as file2, \\\n",
    "        open(file3_path, 'r') as file3, \\\n",
    "        open(file4_path, 'r') as file4, \\\n",
    "        open(file5_path, 'r') as file5, \\\n",
    "        open(file6_path, 'r') as file6, \\\n",
    "        open(file7_path, 'r') as file7, \\\n",
    "        open(new_file_path, 'w', newline='') as new_file:\n",
    "\n",
    "    # Crea oggetto reader per leggere i file CSV esistenti\n",
    "    reader1 = csv.DictReader(file1)\n",
    "    reader2 = csv.DictReader(file2)\n",
    "    reader3 = csv.DictReader(file3)\n",
    "    reader4 = csv.DictReader(file4)\n",
    "    reader5 = csv.DictReader(file5)\n",
    "    reader6 = csv.DictReader(file6)\n",
    "    reader7 = csv.DictReader(file7) \n",
    "\n",
    "    # Crea oggetto writer per scrivere nel nuovo file CSV\n",
    "    fieldnames = ['custody_id', 'incident_id', 'gun_id', 'participant_id', 'geo_id', 'date_id', 'crime_gravity']\n",
    "    writer = csv.DictWriter(new_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Scrivi l'intestazione nel nuovo file CSV\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Itera attraverso le righe dei file CSV e unisci le colonne\n",
    "    for row1, row2, row3, row4, row5, row6, row7 in zip(reader1, reader2, reader3, reader4, reader5, reader6, reader7):\n",
    "        new_row = {\n",
    "            'custody_id': row1['custody_id'],\n",
    "            'incident_id': row2['incident_id'],\n",
    "            'gun_id': row3['gun_id'],\n",
    "            'participant_id': row4['participant_id'],\n",
    "            'geo_id': row5['geo_id'],\n",
    "            'date_id': row6['date_id'],\n",
    "            'crime_gravity': row7['crime_gravity']  \n",
    "        }\n",
    "        # Scrivi la nuova riga nel nuovo file CSV\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "print(\"Completato.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
